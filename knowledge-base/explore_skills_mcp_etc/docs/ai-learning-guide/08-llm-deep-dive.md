# 进阶模块一：LLM 技术深入

> 深入理解大语言模型的核心技术原理

---

## 第24章：Transformer 架构详解

### 24.1 Transformer 的诞生背景

```
为什么 Transformer 革命性？

传统序列模型的问题：

RNN（循环神经网络）：
┌─────────────────────────────────────────────────────────┐
│  输入：["我", "爱", "编", "程"]                         │
│                                                         │
│  处理方式：顺序处理，一个接一个                         │
│  [我] → [爱] → [编] → [程]                             │
│   ↓      ↓      ↓      ↓                               │
│  h1  →  h2  →  h3  →  h4                               │
│                                                         │
│  问题：                                                 │
│  • 无法并行，训练慢                                    │
│  • 长距离依赖困难（梯度消失）                          │
│  • 序列越长，早期信息越容易丢失                        │
└─────────────────────────────────────────────────────────┘

LSTM/GRU 改进：
• 部分解决长距离依赖
• 但仍然无法并行
• 计算复杂度高

Transformer 的突破：
┌─────────────────────────────────────────────────────────┐
│  处理方式：并行处理，同时看所有位置                     │
│                                                         │
│  [我] [爱] [编] [程]                                   │
│    ↓    ↓    ↓    ↓                                    │
│  ┌──────────────────────┐                              │
│  │  Self-Attention      │ ← 每个词同时关注所有词       │
│  └──────────────────────┘                              │
│    ↓    ↓    ↓    ↓                                    │
│  [h1] [h2] [h3] [h4]                                   │
│                                                         │
│  优势：                                                 │
│  • 完全并行化                                          │
│  • 直接建模任意距离的依赖                              │
│  • 训练效率大幅提升                                    │
└─────────────────────────────────────────────────────────┘
```

### 24.2 Self-Attention 机制

```
Self-Attention 是 Transformer 的核心

核心思想：让每个词"关注"序列中的所有词，包括自己

数学表示：
Attention(Q, K, V) = softmax(QK^T / √d_k) × V

三个关键矩阵：
┌─────────────────────────────────────────────────────────┐
│                                                         │
│  Query（查询）：我想找什么信息？                        │
│  Key（键）：我有什么信息可以提供？                      │
│  Value（值）：我实际要传递的信息                        │
│                                                         │
│  类比：图书馆检索                                       │
│  • Query = 你的搜索关键词                              │
│  • Key = 书籍的标签/索引                               │
│  • Value = 书籍的实际内容                              │
│                                                         │
└─────────────────────────────────────────────────────────┘

计算过程详解：

假设输入序列：["我", "爱", "AI"]

步骤 1：生成 Q、K、V
─────────────────────────────────────────
每个词通过三个权重矩阵变换：

词向量 × W_Q = Query
词向量 × W_K = Key  
词向量 × W_V = Value

"我" → Q1, K1, V1
"爱" → Q2, K2, V2
"AI" → Q3, K3, V3

步骤 2：计算注意力分数
─────────────────────────────────────────
每个 Query 与所有 Key 做点积：

        K1    K2    K3
Q1  [  0.8   0.1   0.3  ]  ← "我"关注各词的程度
Q2  [  0.2   0.7   0.5  ]  ← "爱"关注各词的程度
Q3  [  0.1   0.4   0.9  ]  ← "AI"关注各词的程度

步骤 3：Softmax 归一化
─────────────────────────────────────────
将分数转换为概率分布（和为 1）：

        K1    K2    K3
Q1  [  0.6   0.1   0.3  ]  ← 注意力权重
Q2  [  0.15  0.5   0.35 ]
Q3  [  0.1   0.3   0.6  ]

步骤 4：加权求和 Value
─────────────────────────────────────────
输出 = 注意力权重 × Value

"我"的新表示 = 0.6×V1 + 0.1×V2 + 0.3×V3
```

### 24.3 Multi-Head Attention

```
为什么需要多头？

单头注意力的局限：
• 只能学习一种注意力模式
• 可能忽略其他重要的关系

多头注意力的思想：
• 并行运行多个注意力机制
• 每个"头"学习不同的关注模式
• 最后合并所有头的结果

┌─────────────────────────────────────────────────────────┐
│                    Multi-Head Attention                 │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  输入                                                   │
│    │                                                    │
│    ├──────┬──────┬──────┬──────┐                       │
│    ▼      ▼      ▼      ▼      ▼                       │
│  Head1  Head2  Head3  Head4  ...  (通常 8-96 个头)     │
│    │      │      │      │      │                       │
│    │   语法    语义    位置   指代                      │
│    │   关系    相似    关系   关系   ← 不同头学不同模式 │
│    │      │      │      │      │                       │
│    └──────┴──────┴──────┴──────┘                       │
│                    │                                    │
│                    ▼                                    │
│              Concat + Linear                            │
│                    │                                    │
│                    ▼                                    │
│                  输出                                   │
│                                                         │
└─────────────────────────────────────────────────────────┘

实际例子：

句子："小明把书给了小红，她很高兴"

Head 1（指代关系）：
"她" 强烈关注 "小红" → 理解代词指代

Head 2（动作关系）：
"给" 关注 "小明"、"书"、"小红" → 理解谁给谁什么

Head 3（情感关系）：
"高兴" 关注 "给"、"书" → 理解为什么高兴
```

### 24.4 位置编码（Positional Encoding）

```
问题：Transformer 如何知道词的顺序？

Self-Attention 的问题：
• 并行处理，不区分顺序
• "我爱你" 和 "你爱我" 会得到相同结果！

解决方案：位置编码

方式 1：正弦余弦位置编码（原始 Transformer）
─────────────────────────────────────────
PE(pos, 2i) = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))

pos = 位置，i = 维度索引，d = 模型维度

特点：
• 可以处理任意长度的序列
• 相对位置关系可以通过线性变换表示

方式 2：可学习位置编码（GPT、BERT）
─────────────────────────────────────────
• 为每个位置学习一个向量
• 作为参数在训练中优化
• 简单有效，但序列长度固定

方式 3：旋转位置编码 RoPE（LLaMA、Qwen）
─────────────────────────────────────────
• 将位置信息编码到注意力计算中
• 通过旋转矩阵实现
• 更好地捕捉相对位置关系
• 支持长度外推

使用方式：
┌─────────────────────────────────────────────┐
│  词向量 + 位置编码 = 输入表示               │
│                                             │
│  "我" → [0.1, 0.3, ...] + [sin, cos, ...]  │
│           词向量          位置编码          │
│       = [0.2, 0.5, ...]                     │
│         最终输入                            │
└─────────────────────────────────────────────┘
```

### 24.5 完整 Transformer 架构

```
Transformer 的完整结构：

┌─────────────────────────────────────────────────────────┐
│                                                         │
│                     Transformer                         │
│                                                         │
│  ┌─────────────────────┐   ┌─────────────────────┐     │
│  │       Encoder       │   │       Decoder       │     │
│  │                     │   │                     │     │
│  │  ┌───────────────┐  │   │  ┌───────────────┐  │     │
│  │  │ Self-Attention│  │   │  │ Masked Self-  │  │     │
│  │  └───────────────┘  │   │  │   Attention   │  │     │
│  │         ↓           │   │  └───────────────┘  │     │
│  │  ┌───────────────┐  │   │         ↓           │     │
│  │  │ Add & Norm    │  │   │  ┌───────────────┐  │     │
│  │  └───────────────┘  │   │  │ Add & Norm    │  │     │
│  │         ↓           │   │  └───────────────┘  │     │
│  │  ┌───────────────┐  │   │         ↓           │     │
│  │  │   FFN         │  │   │  ┌───────────────┐  │     │
│  │  └───────────────┘  │   │  │Cross-Attention│  │     │
│  │         ↓           │   │  │  (看 Encoder) │  │     │
│  │  ┌───────────────┐  │   │  └───────────────┘  │     │
│  │  │ Add & Norm    │  │   │         ↓           │     │
│  │  └───────────────┘  │   │  ┌───────────────┐  │     │
│  │                     │   │  │ Add & Norm    │  │     │
│  │      × N 层         │   │  └───────────────┘  │     │
│  │                     │   │         ↓           │     │
│  └─────────────────────┘   │  ┌───────────────┐  │     │
│                            │  │   FFN         │  │     │
│                            │  └───────────────┘  │     │
│                            │         ↓           │     │
│                            │  ┌───────────────┐  │     │
│                            │  │ Add & Norm    │  │     │
│                            │  └───────────────┘  │     │
│                            │                     │     │
│                            │      × N 层         │     │
│                            │                     │     │
│                            └─────────────────────┘     │
│                                                         │
└─────────────────────────────────────────────────────────┘

不同模型使用的架构：

Encoder-Only（BERT 类）：
• 只用 Encoder 部分
• 双向注意力，看到所有上下文
• 适合理解任务：分类、NER、问答

Decoder-Only（GPT 类）：★ 现代 LLM 主流
• 只用 Decoder 部分
• 因果注意力，只看左边的词
• 适合生成任务：文本生成、对话

Encoder-Decoder（T5、BART）：
• 完整使用两部分
• 适合序列到序列任务：翻译、摘要
```

---

## 第25章：LLM 训练流程

### 25.1 预训练（Pre-training）

```
预训练是 LLM 获得基础能力的阶段

目标：在海量文本上学习语言的通用模式

数据规模：
┌─────────────────────────────────────────────────────────┐
│  模型           │  训练数据量                           │
├─────────────────────────────────────────────────────────┤
│  GPT-3          │  ~45TB 文本                           │
│  LLaMA 2        │  ~2 万亿 tokens                       │
│  GPT-4          │  未公开（估计更大）                   │
└─────────────────────────────────────────────────────────┘

训练目标：下一个词预测（Next Token Prediction）

输入："今天天气"
目标：预测下一个词是什么

模型看到 "今天天气" → 预测 "很" 的概率最高
模型看到 "今天天气很" → 预测 "好" 的概率最高
...

损失函数：Cross-Entropy Loss
┌─────────────────────────────────────────────────────────┐
│  Loss = -Σ log P(正确的下一个词)                       │
│                                                         │
│  目标：最小化损失，使模型预测正确词的概率最大化        │
└─────────────────────────────────────────────────────────┘

预训练的计算成本：
┌─────────────────────────────────────────────────────────┐
│  模型           │  估计训练成本                         │
├─────────────────────────────────────────────────────────┤
│  GPT-3 (175B)   │  ~$4.6M（2020年）                     │
│  GPT-4          │  ~$100M+（估计）                      │
│  LLaMA 2 70B    │  ~$2M                                 │
└─────────────────────────────────────────────────────────┘
```

### 25.2 后训练（Post-training）

```
预训练后的模型需要进一步调整才能好用

阶段 1：监督微调（SFT - Supervised Fine-Tuning）
─────────────────────────────────────────────────────────

用高质量的问答对微调模型

数据格式：
{
  "instruction": "解释什么是机器学习",
  "response": "机器学习是人工智能的一个分支..."
}

目的：
• 让模型学会遵循指令
• 学习对话格式
• 提升特定任务能力

数据来源：
• 人工标注
• 优质对话数据
• 任务特定数据

阶段 2：人类反馈强化学习（RLHF）
─────────────────────────────────────────────────────────

让模型输出更符合人类偏好

流程：
┌─────────────────────────────────────────────────────────┐
│                                                         │
│  1. 收集人类偏好数据                                   │
│     ─────────────────                                   │
│     问题："如何学编程？"                               │
│     回答A："首先选择一门语言..."（人类评分：8/10）    │
│     回答B："编程很简单..."（人类评分：5/10）          │
│     → 人类更喜欢 A                                     │
│                                                         │
│  2. 训练奖励模型（Reward Model）                       │
│     ─────────────────                                   │
│     学习预测人类偏好                                   │
│     输入：问题 + 回答 → 输出：分数                    │
│                                                         │
│  3. 用 PPO 优化 LLM                                    │
│     ─────────────────                                   │
│     LLM 生成回答 → 奖励模型打分 → 优化 LLM            │
│     目标：生成高分回答                                 │
│                                                         │
└─────────────────────────────────────────────────────────┘

阶段 3：直接偏好优化（DPO）
─────────────────────────────────────────────────────────

RLHF 的简化替代方案

优势：
• 不需要单独训练奖励模型
• 训练更稳定
• 实现更简单

原理：
直接用偏好数据优化模型，无需强化学习
```

### 25.3 Scaling Laws（规模定律）

```
更大的模型 + 更多数据 = 更好的效果

Chinchilla Scaling Laws（2022）：
┌─────────────────────────────────────────────────────────┐
│                                                         │
│  最优训练：模型参数量 ≈ 训练 token 数 / 20            │
│                                                         │
│  示例：                                                 │
│  • 10B 参数模型 → 需要 ~200B tokens                    │
│  • 70B 参数模型 → 需要 ~1.4T tokens                    │
│                                                         │
│  含义：                                                 │
│  • 之前的模型普遍"欠训练"                             │
│  • 数据量和模型大小需要平衡                            │
│  • 小模型训练充分后，效果可能超过大模型                │
│                                                         │
└─────────────────────────────────────────────────────────┘

涌现能力（Emergent Abilities）：

当模型规模超过某个阈值，突然出现新能力

┌─────────────────────────────────────────────────────────┐
│  能力           │  涌现阈值（大约）                     │
├─────────────────────────────────────────────────────────┤
│  思维链推理     │  ~100B 参数                           │
│  复杂数学      │  ~100B+ 参数                          │
│  代码生成      │  ~10B+ 参数                           │
└─────────────────────────────────────────────────────────┘

这也是为什么大模型比小模型"聪明"很多
```

### 25.4 推理优化技术

```
如何让大模型跑得更快更省资源？

技术 1：量化（Quantization）
─────────────────────────────────────────
将模型权重从高精度转为低精度

FP32 → FP16 → INT8 → INT4

┌─────────────────────────────────────────────────────────┐
│  精度       │  内存占用  │  速度      │  质量损失     │
├─────────────────────────────────────────────────────────┤
│  FP32       │  100%      │  基准      │  无           │
│  FP16/BF16  │  50%       │  ~2x 快    │  很小        │
│  INT8       │  25%       │  ~3x 快    │  小          │
│  INT4       │  12.5%     │  ~4x 快    │  中等        │
└─────────────────────────────────────────────────────────┘

常见量化方法：
• GPTQ：训练后量化，INT4
• AWQ：激活感知量化，质量更好
• GGUF/GGML：llama.cpp 使用的格式

技术 2：KV Cache
─────────────────────────────────────────
缓存已计算的 Key 和 Value，避免重复计算

生成第 N 个词时：
• 不缓存：重新计算前 N-1 个词的 KV
• 有缓存：直接用缓存的 KV，只计算新词

效果：生成速度大幅提升

技术 3：Flash Attention
─────────────────────────────────────────
优化注意力计算的内存访问模式

• 减少 GPU 内存读写
• 支持更长的上下文
• 训练和推理都更快

技术 4：推测解码（Speculative Decoding）
─────────────────────────────────────────
用小模型先生成草稿，大模型验证

流程：
1. 小模型快速生成 N 个词
2. 大模型并行验证这 N 个词
3. 接受正确的，从错误处继续

效果：在保持质量的同时加速 2-3 倍

技术 5：Continuous Batching
─────────────────────────────────────────
动态批处理，不同请求可以随时加入/退出

传统：等一批请求都完成才处理下一批
Continuous：请求完成就立即处理新请求

效果：吞吐量提升 2-5 倍
```

---

## 第26章：模型架构演进

### 26.1 主流架构对比

```
现代 LLM 的架构选择：

┌─────────────────────────────────────────────────────────┐
│                    架构对比                             │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  GPT 系列架构                                           │
│  ─────────────                                          │
│  • Decoder-only                                         │
│  • 可学习位置编码                                      │
│  • Pre-LayerNorm                                        │
│  • 标准 Multi-Head Attention                           │
│                                                         │
│  LLaMA 系列架构                                        │
│  ─────────────                                          │
│  • Decoder-only                                         │
│  • RoPE 位置编码（支持长度外推）                       │
│  • Pre-LayerNorm (RMSNorm)                             │
│  • Grouped-Query Attention (GQA)                       │
│  • SwiGLU 激活函数                                     │
│                                                         │
│  Mistral/Mixtral 架构                                  │
│  ─────────────────                                      │
│  • 基于 LLaMA                                          │
│  • Sliding Window Attention（滑动窗口）                │
│  • MoE（Mixture of Experts，专家混合）                 │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 26.2 Grouped-Query Attention (GQA)

```
GQA 是优化 KV Cache 内存的关键技术

Multi-Head Attention (MHA) 的问题：
• 每个注意力头都有独立的 K 和 V
• KV Cache 占用大量内存
• 限制了可处理的上下文长度

解决方案演进：

1. Multi-Head Attention (MHA)
   ┌─────────────────────────────────────────┐
   │  Q: H 个头                              │
   │  K: H 个头  ← 内存占用大               │
   │  V: H 个头  ← 内存占用大               │
   └─────────────────────────────────────────┘

2. Multi-Query Attention (MQA)
   ┌─────────────────────────────────────────┐
   │  Q: H 个头                              │
   │  K: 1 个头  ← 大幅减少                 │
   │  V: 1 个头  ← 大幅减少                 │
   │                                         │
   │  问题：质量可能下降                     │
   └─────────────────────────────────────────┘

3. Grouped-Query Attention (GQA) ★ 现代主流
   ┌─────────────────────────────────────────┐
   │  Q: H 个头                              │
   │  K: G 个头  ← 折中方案（如 8 个）      │
   │  V: G 个头  ← 每组共享 KV              │
   │                                         │
   │  优势：平衡质量和效率                   │
   └─────────────────────────────────────────┘

LLaMA 2 70B 使用 GQA：
• 64 个 Query 头
• 8 个 KV 头（8 组，每组 8 个 Q 头共享 1 个 KV）
• KV Cache 减少 8 倍
```

### 26.3 Mixture of Experts (MoE)

```
MoE 让模型更大但计算量不增加

传统 Dense 模型：
┌─────────────────────────────────────────────────────────┐
│  输入 → 所有参数都参与计算 → 输出                      │
│                                                         │
│  问题：参数越多，计算量越大                            │
└─────────────────────────────────────────────────────────┘

MoE 模型：
┌─────────────────────────────────────────────────────────┐
│                                                         │
│  输入                                                   │
│    │                                                    │
│    ▼                                                    │
│  ┌─────────────────┐                                   │
│  │  Router（路由） │ ← 决定激活哪些专家                │
│  └────────┬────────┘                                   │
│           │                                             │
│     ┌─────┴─────┐                                      │
│     ▼           ▼                                       │
│  ┌──────┐   ┌──────┐   ┌──────┐   ┌──────┐           │
│  │Expert│   │Expert│   │Expert│   │Expert│ ...        │
│  │  1   │   │  2   │   │  3   │   │  4   │            │
│  └──────┘   └──────┘   └──────┘   └──────┘            │
│     │           │                                       │
│     └─────┬─────┘       只激活部分专家                  │
│           │             （如 8 选 2）                   │
│           ▼                                             │
│         输出                                            │
│                                                         │
└─────────────────────────────────────────────────────────┘

Mixtral 8x7B 示例：
• 8 个专家，每次激活 2 个
• 总参数：~47B
• 每次推理激活：~13B
• 效果接近 70B Dense 模型，但更快

MoE 的优势：
✅ 参数量大，能力强
✅ 实际计算量小，推理快
✅ 训练成本相对较低

MoE 的挑战：
❌ 需要更多显存存储所有专家
❌ 负载均衡问题（某些专家被过度使用）
❌ 通信开销（分布式训练）
```

---

## 第27章：长上下文技术

### 27.1 上下文长度的挑战

```
为什么长上下文很难？

问题 1：注意力复杂度
─────────────────────────────────────────
Self-Attention 复杂度：O(n²)

n = 序列长度
• 4K tokens: 1600 万次运算
• 32K tokens: 10 亿次运算
• 128K tokens: 160 亿次运算

长度增加 4 倍，计算量增加 16 倍！

问题 2：KV Cache 内存
─────────────────────────────────────────
每层需要缓存 K 和 V

LLaMA 2 7B 示例：
• 4K 上下文：~1 GB KV Cache
• 32K 上下文：~8 GB KV Cache
• 128K 上下文：~32 GB KV Cache

问题 3：位置编码外推
─────────────────────────────────────────
训练时见过的最大位置 vs 推理时需要的位置

训练：最大 4K
推理：想处理 32K
→ 超出训练范围，效果下降
```

### 27.2 长上下文解决方案

```
方案 1：位置编码改进
─────────────────────────────────────────

RoPE 插值（Position Interpolation）：
• 将超长位置"压缩"到训练过的范围内
• 简单有效，广泛使用

YaRN（Yet another RoPE extensioN）：
• 更复杂的插值方案
• 效果更好

NTK-aware 插值：
• 针对不同频率分别处理
• 保留高频信息

方案 2：稀疏注意力
─────────────────────────────────────────

Sliding Window Attention：
┌─────────────────────────────────────────────────────────┐
│  不是关注所有位置，只关注附近的窗口                     │
│                                                         │
│  位置 1  2  3  4  5  6  7  8  9  10 ...                │
│                                                         │
│  位置 5 只关注：[3, 4, 5, 6, 7]（窗口=5）              │
│                                                         │
│  复杂度：O(n × w) 而不是 O(n²)                         │
│                                                         │
│  Mistral 使用此方案                                     │
└─────────────────────────────────────────────────────────┘

方案 3：分层处理
─────────────────────────────────────────

Memory/Retrieval Augmented：
• 不是把所有内容放进上下文
• 而是检索相关部分
• 类似 RAG 的思想

方案 4：线性注意力
─────────────────────────────────────────

将 O(n²) 降为 O(n)

代表工作：
• Linear Attention
• Mamba（State Space Model）
• RWKV

trade-off：速度提升但效果可能略降
```

### 27.3 实际模型的长上下文能力

```
主流模型的上下文长度（2025年）：

┌─────────────────────────────────────────────────────────┐
│  模型                    │  上下文长度                  │
├─────────────────────────────────────────────────────────┤
│  GPT-4 Turbo             │  128K tokens                 │
│  Claude 3                │  200K tokens                 │
│  Gemini 1.5 Pro          │  1M tokens                   │
│  LLaMA 3 (8B/70B)        │  8K-128K tokens              │
│  Qwen 2.5                │  128K tokens                 │
│  Mistral Large           │  128K tokens                 │
└─────────────────────────────────────────────────────────┘

注意：上下文长度 ≠ 有效利用长度

"大海捞针"测试显示：
• 很多模型在超长上下文中会"遗忘"中间内容
• 开头和结尾的信息被更好地记住
• 实际有效利用的长度可能远小于标称值
```

---

## 本章小结

```
核心要点回顾：

1. Transformer 架构
   ├── Self-Attention：核心机制，计算词间关系
   ├── Multi-Head：多个注意力头学习不同模式
   ├── 位置编码：让模型理解词的顺序
   └── 现代 LLM 主要使用 Decoder-only 架构

2. LLM 训练流程
   ├── 预训练：海量数据上学习语言模式
   ├── SFT：学会遵循指令
   ├── RLHF/DPO：对齐人类偏好
   └── Scaling Laws：更大更好

3. 推理优化
   ├── 量化：INT8/INT4 减少内存和加速
   ├── KV Cache：避免重复计算
   ├── Flash Attention：优化内存访问
   └── 推测解码：小模型草稿+大模型验证

4. 架构演进
   ├── GQA：减少 KV Cache 内存
   ├── MoE：更大参数但计算量不增加
   └── 长上下文：位置编码改进、稀疏注意力
```

---

**下一章 → [第28章：Agent 开发框架](./09-agent-development.md)**
