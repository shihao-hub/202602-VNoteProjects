# 模块六：RAG 与向量数据库

> 让 AI 能够查阅和使用你的知识

---

## 第20章：RAG 原理与架构

### 20.1 RAG（检索增强生成）定义

**RAG（Retrieval-Augmented Generation，检索增强生成）** 是一种让 AI 在回答问题前先"查资料"的技术。

```
简单理解：

普通 LLM 回答问题：
┌─────────────────────────────────────────────┐
│  用户：公司的报销流程是什么？               │
│                                             │
│  LLM：（只能用训练时学到的通用知识）       │
│       "一般来说，报销流程包括..."          │
│       （无法回答公司特定的流程）            │
└─────────────────────────────────────────────┘

RAG 增强的 LLM 回答问题：
┌─────────────────────────────────────────────┐
│  用户：公司的报销流程是什么？               │
│                                             │
│  RAG 系统：                                 │
│  1. [搜索公司知识库]                        │
│  2. [找到《员工报销指南》]                  │
│  3. [把相关内容提供给 LLM]                  │
│                                             │
│  LLM：（有了参考资料）                      │
│       "根据公司规定，报销流程如下：        │
│        1. 填写报销申请表                    │
│        2. 提交给直属主管审批                │
│        3. ..."                              │
└─────────────────────────────────────────────┘
```

### 20.2 为什么需要 RAG

```
RAG 解决的问题：

问题 1：知识截止
─────────────────────────────────────────
LLM 的训练数据有时间限制
RAG 可以实时检索最新信息

问题 2：私有知识
─────────────────────────────────────────
LLM 不知道你公司的内部文档、私有数据
RAG 可以让 LLM 访问这些知识

问题 3：准确性
─────────────────────────────────────────
LLM 可能产生幻觉
RAG 提供真实的参考资料，减少幻觉

问题 4：可追溯性
─────────────────────────────────────────
RAG 可以告诉你答案来自哪个文档
便于验证和追溯

RAG vs Fine-tuning（微调）：
┌────────────────────────────────────────────────────┐
│ 特性         │ RAG              │ Fine-tuning      │
├────────────────────────────────────────────────────┤
│ 知识更新     │ 即时更新         │ 需要重新训练     │
│ 成本         │ 较低             │ 较高             │
│ 准确性       │ 可追溯，易验证   │ 难以验证         │
│ 适用场景     │ 知识密集型问答   │ 风格/行为调整    │
│ 隐私         │ 数据不需入模型   │ 数据需入模型     │
└────────────────────────────────────────────────────┘
```

### 20.3 RAG 的基本流程

```
RAG 的工作流程：

┌─────────────────────────────────────────────────────────┐
│                                                         │
│  ┌──────────────┐                                      │
│  │  用户问题    │                                      │
│  └──────┬───────┘                                      │
│         │                                               │
│         ▼                                               │
│  ┌──────────────┐      ┌──────────────────────────┐   │
│  │  问题向量化  │      │       知识库             │   │
│  │  (Embedding) │      │  ┌────────────────────┐  │   │
│  └──────┬───────┘      │  │ 文档1 → 向量1     │  │   │
│         │              │  │ 文档2 → 向量2     │  │   │
│         │              │  │ 文档3 → 向量3     │  │   │
│         │              │  │ ...               │  │   │
│         ▼              │  └────────────────────┘  │   │
│  ┌──────────────┐      └──────────┬───────────────┘   │
│  │  相似度搜索  │ ◀───────────────┘                   │
│  └──────┬───────┘                                      │
│         │                                               │
│         ▼                                               │
│  ┌──────────────┐                                      │
│  │  检索结果    │ ← 最相关的文档片段                  │
│  └──────┬───────┘                                      │
│         │                                               │
│         ▼                                               │
│  ┌───────────────────────────────────────────┐         │
│  │              构造 Prompt                   │         │
│  │  ┌─────────────────────────────────────┐ │         │
│  │  │ 参考资料：[检索到的内容]            │ │         │
│  │  │ 用户问题：[原始问题]                │ │         │
│  │  │ 请根据参考资料回答问题。            │ │         │
│  │  └─────────────────────────────────────┘ │         │
│  └──────┬────────────────────────────────────┘         │
│         │                                               │
│         ▼                                               │
│  ┌──────────────┐                                      │
│  │     LLM      │                                      │
│  └──────┬───────┘                                      │
│         │                                               │
│         ▼                                               │
│  ┌──────────────┐                                      │
│  │   回答用户   │                                      │
│  └──────────────┘                                      │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 20.4 类比理解

> **RAG 让 AI "能查资料再回答"**

```
类比：开卷考试 vs 闭卷考试

闭卷考试（普通 LLM）：
┌─────────────────────────────────────────────┐
│ 考生只能凭记忆作答                          │
│ • 优点：回答快                              │
│ • 缺点：可能记错，无法回答没学过的内容      │
└─────────────────────────────────────────────┘

开卷考试（RAG）：
┌─────────────────────────────────────────────┐
│ 考生可以翻阅资料                            │
│ • 优点：答案更准确，可以处理新知识          │
│ • 缺点：需要时间查找                        │
└─────────────────────────────────────────────┘

更贴切的类比：图书馆管理员

用户："我想了解量子计算的最新进展"

普通管理员（LLM）：
"根据我几年前学到的知识..."

RAG 管理员：
1. "让我帮你查一下..."
2. [走进图书馆，搜索相关书籍]
3. [找到最新的量子计算论文和书籍]
4. [阅读相关段落]
5. "根据这本最新出版的书，量子计算的最新进展是..."
```

---

## 第21章：向量嵌入（Embedding）基础

### 21.1 什么是向量嵌入

**向量嵌入（Vector Embedding）** 是将文本（或图像、音频等）转换为数字向量的技术，使得语义相似的内容在向量空间中距离相近。

```
基本概念：

文本 → Embedding 模型 → 向量

示例：
"我喜欢吃苹果" → [0.12, -0.34, 0.56, ..., 0.78]
                    └─────── 768维或更多 ───────┘

为什么要转换成向量？
• 计算机不理解文字的"含义"
• 但可以计算向量之间的距离
• 距离近 = 语义相似
```

### 21.2 文本如何变成向量

```
Embedding 模型的工作原理（简化）：

┌─────────────────────────────────────────────────────────┐
│                                                         │
│  输入文本："机器学习入门"                              │
│       │                                                 │
│       ▼                                                 │
│  ┌──────────────────────────────────────┐              │
│  │          Embedding 模型              │              │
│  │  (如 OpenAI text-embedding-ada-002)  │              │
│  │                                      │              │
│  │  内部过程（你不需要关心细节）：       │              │
│  │  • Tokenization（分词）              │              │
│  │  • 神经网络处理                      │              │
│  │  • 输出固定长度的向量                │              │
│  └──────────────────────────────────────┘              │
│       │                                                 │
│       ▼                                                 │
│  输出向量：[0.023, -0.156, 0.089, ..., 0.234]         │
│           └──────────── 1536 维 ──────────────┘        │
│                                                         │
└─────────────────────────────────────────────────────────┘

向量的维度：
• 不同模型输出不同维度
• OpenAI ada-002: 1536 维
• Cohere: 1024 或 4096 维
• 开源模型: 384, 768, 1024 等

维度越高：
• 通常能捕捉更多语义信息
• 但计算和存储成本更高
```

### 21.3 语义相似度与向量距离

```
向量距离 = 语义相似度

常用的距离计算方法：

1. 余弦相似度（Cosine Similarity）—— 最常用
   • 计算两个向量夹角的余弦值
   • 范围：-1 到 1（1 表示完全相同）
   • 不受向量长度影响
   
   示例：
   "我爱吃苹果" 和 "我喜欢苹果" → 相似度 0.95（非常相似）
   "我爱吃苹果" 和 "今天天气真好" → 相似度 0.12（不相似）

2. 欧氏距离（Euclidean Distance）
   • 向量空间中的直线距离
   • 值越小越相似

3. 点积（Dot Product）
   • 计算简单，常用于优化
   • 需要归一化处理

语义搜索的原理：
┌─────────────────────────────────────────────────────────┐
│                                                         │
│  查询："如何学习编程？"                                │
│         │                                               │
│         ▼ Embedding                                     │
│  查询向量：[0.1, 0.3, -0.2, ...]                       │
│         │                                               │
│         │ 计算与知识库中所有向量的距离                  │
│         │                                               │
│         ▼                                               │
│  ┌─────────────────────────────────────────┐           │
│  │ 文档1 "Python入门教程"     相似度: 0.89 │ ✅ 最相似 │
│  │ 文档2 "JavaScript学习指南" 相似度: 0.85 │ ✅ 相似   │
│  │ 文档3 "美食推荐"          相似度: 0.12 │ ❌ 不相关 │
│  └─────────────────────────────────────────┘           │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 21.4 Embedding 模型介绍

```
常用 Embedding 模型：

┌─────────────────────────────────────────────────────────┐
│                 Embedding 模型对比                      │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  OpenAI 系列                                            │
│  ───────────                                            │
│  • text-embedding-ada-002 (1536维)                      │
│  • text-embedding-3-small (512/1536维)                  │
│  • text-embedding-3-large (256-3072维)                  │
│  优点：效果好，使用简单                                │
│  缺点：需要 API 调用，有成本                           │
│                                                         │
│  Cohere                                                 │
│  ──────                                                 │
│  • embed-english-v3.0                                   │
│  • embed-multilingual-v3.0                              │
│  优点：多语言支持好                                    │
│                                                         │
│  开源模型                                               │
│  ────────                                               │
│  • sentence-transformers/all-MiniLM-L6-v2 (384维)      │
│  • BAAI/bge-large-en (1024维)                          │
│  • intfloat/multilingual-e5-large (1024维)             │
│  优点：免费，可本地部署                                │
│  缺点：需要自己部署和维护                              │
│                                                         │
│  选择建议：                                             │
│  • 快速原型 → OpenAI                                   │
│  • 生产环境 → 根据成本和效果选择                       │
│  • 隐私要求高 → 开源模型本地部署                       │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

---

## 第22章：向量数据库介绍

### 22.1 向量数据库的作用

```
为什么需要向量数据库？

普通数据库 vs 向量数据库：

普通数据库（如 MySQL）：
┌─────────────────────────────────────────────┐
│ SELECT * FROM docs WHERE title = '机器学习'│
│                                             │
│ 只能精确匹配或简单模糊匹配                  │
│ 无法理解语义                                │
└─────────────────────────────────────────────┘

向量数据库：
┌─────────────────────────────────────────────┐
│ 搜索与 [0.1, 0.3, ...] 最相似的 10 个向量  │
│                                             │
│ 基于语义相似度搜索                          │
│ "AI入门" 能找到 "机器学习基础"             │
└─────────────────────────────────────────────┘

向量数据库的核心功能：
• 存储向量和关联的元数据
• 高效的相似度搜索（即使有百万级向量）
• 支持过滤和混合查询
```

### 22.2 常见向量数据库

```
主流向量数据库对比：

┌─────────────────────────────────────────────────────────┐
│                                                         │
│  Pinecone                                               │
│  ────────                                               │
│  类型：云服务                                           │
│  特点：                                                 │
│  • 全托管，无需运维                                    │
│  • 开箱即用                                            │
│  • 按使用量付费                                        │
│  适合：快速开始，不想管基础设施                        │
│                                                         │
│  ─────────────────────────────────────────────────     │
│                                                         │
│  Milvus                                                 │
│  ──────                                                 │
│  类型：开源，可自托管或云服务                          │
│  特点：                                                 │
│  • 功能强大，性能好                                    │
│  • 支持大规模数据                                      │
│  • 社区活跃                                            │
│  适合：生产环境，大规模应用                            │
│                                                         │
│  ─────────────────────────────────────────────────     │
│                                                         │
│  Chroma                                                 │
│  ──────                                                 │
│  类型：开源，嵌入式/客户端-服务器                      │
│  特点：                                                 │
│  • 轻量级，易于使用                                    │
│  • Python 友好                                         │
│  • 适合原型开发                                        │
│  适合：学习、原型、小规模应用                          │
│                                                         │
│  ─────────────────────────────────────────────────     │
│                                                         │
│  FAISS（Facebook AI Similarity Search）                │
│  ──────                                                 │
│  类型：开源库（不是数据库）                            │
│  特点：                                                 │
│  • 高性能相似度搜索                                    │
│  • 需要自己管理存储                                    │
│  • 适合研究和定制                                      │
│  适合：需要最高性能，愿意自己管理                      │
│                                                         │
│  ─────────────────────────────────────────────────     │
│                                                         │
│  Qdrant                                                 │
│  ──────                                                 │
│  类型：开源，可自托管或云服务                          │
│  特点：                                                 │
│  • Rust 编写，性能好                                   │
│  • 支持丰富的过滤条件                                  │
│  • API 友好                                            │
│  适合：需要复杂过滤的场景                              │
│                                                         │
│  ─────────────────────────────────────────────────     │
│                                                         │
│  Weaviate                                               │
│  ────────                                               │
│  类型：开源，可自托管或云服务                          │
│  特点：                                                 │
│  • 内置向量化模块                                      │
│  • GraphQL 查询接口                                    │
│  • 支持混合搜索                                        │
│  适合：想要一体化解决方案                              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 22.3 向量索引与检索原理

```
为什么向量搜索能这么快？

暴力搜索的问题：
• 100万个向量，每次搜索要比较100万次
• 每次比较计算1536维向量的距离
• 太慢了！

解决方案：向量索引

常见索引类型：

1. IVF（Inverted File Index）
   ─────────────────────────────
   思想：把向量分成多个簇，搜索时只在相关簇中查找
   
   ┌─────────────────────────────────────┐
   │         向量空间                    │
   │    ┌────┐  ┌────┐  ┌────┐         │
   │    │簇1 │  │簇2 │  │簇3 │  ...    │
   │    │••• │  │••• │  │••• │         │
   │    └────┘  └────┘  └────┘         │
   │                                     │
   │   搜索时：                          │
   │   1. 找到最近的几个簇               │
   │   2. 只在这些簇内精确搜索           │
   └─────────────────────────────────────┘

2. HNSW（Hierarchical Navigable Small World）
   ─────────────────────────────
   思想：构建多层图结构，通过图导航快速找到相似向量
   
   • 像走迷宫，每一步都往目标靠近
   • 非常快，效果好
   • 是目前最流行的索引方法

3. 量化（Quantization）
   ─────────────────────────────
   思想：压缩向量，用近似值换取速度和空间
   
   • Product Quantization (PQ)
   • Scalar Quantization (SQ)
   • 以少量精度损失换取大幅性能提升

实际选择：
• 小数据量（<10万）：暴力搜索就够了
• 中等数据量：IVF 或 HNSW
• 大数据量：HNSW + 量化
```

### 22.4 如何选择向量数据库

```
选择决策树：

                    ┌─────────────────┐
                    │ 你的使用场景？   │
                    └────────┬────────┘
                             │
      ┌──────────────────────┼──────────────────────┐
      │                      │                      │
      ▼                      ▼                      ▼
 ┌──────────┐         ┌──────────┐         ┌──────────┐
 │学习/原型 │         │ 生产环境 │         │ 大规模   │
 └────┬─────┘         └────┬─────┘         └────┬─────┘
      │                    │                    │
      ▼                    ▼                    ▼
  Chroma             ┌─────┴─────┐           Milvus
  (简单易用)         │           │         Pinecone
                     ▼           ▼        (大规模优化)
                  不想运维    可以运维
                     │           │
                     ▼           ▼
                  Pinecone    Qdrant
                  (托管)     Weaviate
                            Milvus

具体建议：

┌────────────────────────────────────────────────────────┐
│ 场景                    │ 推荐                        │
├────────────────────────────────────────────────────────┤
│ 学习和实验              │ Chroma（最简单）            │
│ 个人项目/小团队         │ Chroma 或 Qdrant           │
│ 生产环境，不想运维      │ Pinecone                   │
│ 生产环境，可以运维      │ Milvus 或 Qdrant           │
│ 需要最高性能            │ FAISS + 自己管理           │
│ 需要混合搜索            │ Weaviate                   │
└────────────────────────────────────────────────────────┘
```

---

## 第23章：RAG 应用实践

### 23.1 构建知识库的流程

```
完整的 RAG 知识库构建流程：

┌─────────────────────────────────────────────────────────┐
│                                                         │
│  步骤 1：准备文档                                       │
│  ─────────────────                                      │
│  • 收集所有相关文档                                    │
│  • 支持的格式：PDF、Word、Markdown、HTML 等            │
│  • 确保文档质量（内容准确、格式清晰）                  │
│                                                         │
│         │                                               │
│         ▼                                               │
│                                                         │
│  步骤 2：文档解析                                       │
│  ─────────────────                                      │
│  • 提取文本内容                                        │
│  • 处理表格、图片（可选）                              │
│  • 保留必要的元数据（标题、来源、日期）                │
│                                                         │
│         │                                               │
│         ▼                                               │
│                                                         │
│  步骤 3：文档分块（Chunking）                          │
│  ─────────────────────────                              │
│  • 将长文档切分成小段                                  │
│  • 常见策略：                                          │
│    - 固定长度分块（如每 500 字）                       │
│    - 基于段落分块                                      │
│    - 基于语义分块                                      │
│  • 保持上下文连贯性（适当重叠）                        │
│                                                         │
│         │                                               │
│         ▼                                               │
│                                                         │
│  步骤 4：生成向量                                       │
│  ─────────────────                                      │
│  • 调用 Embedding 模型                                 │
│  • 每个文档块 → 一个向量                              │
│  • 批量处理提高效率                                    │
│                                                         │
│         │                                               │
│         ▼                                               │
│                                                         │
│  步骤 5：存入向量数据库                                │
│  ─────────────────────                                  │
│  • 向量 + 原文 + 元数据一起存储                       │
│  • 建立索引                                            │
│                                                         │
│         │                                               │
│         ▼                                               │
│                                                         │
│  步骤 6：测试和优化                                    │
│  ─────────────────                                      │
│  • 用测试问题验证效果                                  │
│  • 调整参数（分块大小、检索数量等）                    │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 23.2 文档分块策略

```
分块是 RAG 效果的关键因素之一

常见分块策略：

1. 固定长度分块
   ─────────────────────────────
   方法：按字符数或 Token 数切分
   
   原文档：[========================================]
   分块后：[=====][=====][=====][=====][=====][===]
          chunk1 chunk2 chunk3 chunk4 chunk5 chunk6
   
   优点：简单直接
   缺点：可能切断句子或段落

2. 重叠分块（Overlap）
   ─────────────────────────────
   方法：相邻块之间有部分重叠
   
   [========]
       [========]
           [========]
               [========]
   
   优点：保持上下文连贯性
   缺点：存储空间增加

3. 基于分隔符分块
   ─────────────────────────────
   方法：按段落、章节、或其他分隔符切分
   
   ## 第一章          → chunk1
   段落1...           
   段落2...           
   
   ## 第二章          → chunk2
   段落1...
   
   优点：语义更完整
   缺点：块大小不均匀

4. 递归分块
   ─────────────────────────────
   方法：先按大分隔符分，如果太大再按小分隔符分
   
   分隔符优先级：章节 > 段落 > 句子 > 字符
   
   优点：灵活，适应性强
   是 LangChain 等框架的默认方式

分块大小建议：
┌────────────────────────────────────────────────┐
│ 场景              │ 建议大小                   │
├────────────────────────────────────────────────┤
│ 问答系统          │ 200-500 tokens            │
│ 文档摘要          │ 500-1000 tokens           │
│ 代码搜索          │ 按函数/类分块              │
│ 对话历史          │ 按对话轮次分块             │
└────────────────────────────────────────────────┘

块太小：缺少上下文，检索不准
块太大：包含无关内容，干扰 LLM
```

### 23.3 检索优化技巧

```
提升 RAG 检索效果的技巧：

技巧 1：优化查询
─────────────────────────────
问题：用户问题可能不是最佳的搜索查询
解决：用 LLM 改写查询

用户："怎么让程序跑得更快？"
改写后：
- "性能优化技术"
- "代码优化方法"  
- "提高程序执行效率"

技巧 2：混合检索（Hybrid Search）
─────────────────────────────
结合向量搜索和关键词搜索

┌─────────────────────────────────────────┐
│         用户查询                        │
│            │                            │
│     ┌──────┴──────┐                    │
│     ▼             ▼                    │
│ [向量搜索]    [关键词搜索]             │
│  语义相似      精确匹配                 │
│     │             │                    │
│     └──────┬──────┘                    │
│            ▼                            │
│      [融合排序]                         │
│            │                            │
│            ▼                            │
│      [最终结果]                         │
└─────────────────────────────────────────┘

技巧 3：重排序（Re-ranking）
─────────────────────────────
先检索多一些，再用更精确的模型重排

1. 向量搜索：检索 Top 50
2. Re-ranker：重新排序，选 Top 5
3. 发送给 LLM

Re-ranker 模型：Cohere Rerank、BGE Reranker 等

技巧 4：元数据过滤
─────────────────────────────
利用元数据缩小搜索范围

用户："2024年的财务报告里说了什么？"
查询时：
- 向量相似度搜索
- 同时过滤：year = 2024 AND type = "财务报告"

技巧 5：多路召回
─────────────────────────────
从不同角度检索，合并结果

用户："Python 如何处理 JSON？"
多路召回：
- 召回1：搜索 "Python"
- 召回2：搜索 "JSON"
- 召回3：搜索 "Python JSON 处理"
合并去重后返回
```

### 23.4 RAG 系统的评估与调优

```
评估 RAG 系统的指标：

1. 检索质量指标
   ─────────────────────────────
   • Recall@K：相关文档在 Top K 中的比例
   • Precision@K：Top K 中相关文档的比例
   • MRR（Mean Reciprocal Rank）：第一个相关文档的排名
   
   简单测试方法：
   准备 50 个测试问题 + 标准答案
   检查检索到的内容是否包含答案所需信息

2. 生成质量指标
   ─────────────────────────────
   • 答案准确性：回答是否正确
   • 答案相关性：是否回答了用户的问题
   • 答案完整性：是否提供了足够的信息
   • 忠实度：是否与检索到的内容一致（不产生幻觉）

3. 系统性能指标
   ─────────────────────────────
   • 延迟：从提问到返回答案的时间
   • 吞吐量：每秒能处理的请求数
   • 成本：API 调用费用

调优策略：
┌─────────────────────────────────────────────────────────┐
│ 问题                  │ 可能的解决方案                  │
├─────────────────────────────────────────────────────────┤
│ 检索不到相关内容      │ • 检查分块策略                  │
│                       │ • 换更好的 Embedding 模型       │
│                       │ • 添加混合检索                  │
├─────────────────────────────────────────────────────────┤
│ 检索到但答案不对      │ • 检查 Prompt 模板              │
│                       │ • 增加检索数量                  │
│                       │ • 添加 Re-ranking               │
├─────────────────────────────────────────────────────────┤
│ 答案包含幻觉          │ • 强调只根据参考资料回答        │
│                       │ • 要求引用来源                  │
│                       │ • 降低 LLM 温度                 │
├─────────────────────────────────────────────────────────┤
│ 响应太慢              │ • 减少检索数量                  │
│                       │ • 使用更快的向量索引            │
│                       │ • 缓存常见查询                  │
└─────────────────────────────────────────────────────────┘
```

---

## 本章小结

```
核心要点回顾：

1. RAG 原理
   ├── 检索增强生成：先查资料，再回答
   ├── 解决 LLM 的知识截止和私有数据问题
   └── 让 AI 能够使用你的知识库

2. 向量嵌入
   ├── 将文本转换为数字向量
   ├── 语义相似的内容在向量空间中距离近
   ├── 是 RAG 检索的基础
   └── 常用模型：OpenAI、Cohere、开源模型

3. 向量数据库
   ├── 专门存储和检索向量的数据库
   ├── 支持高效的相似度搜索
   ├── 常见选择：Pinecone、Milvus、Chroma、Qdrant
   └── 根据场景选择托管或自托管

4. RAG 实践
   ├── 知识库构建：文档 → 分块 → 向量化 → 存储
   ├── 分块策略：大小、重叠、分隔符
   ├── 检索优化：查询改写、混合检索、重排序
   └── 评估调优：检索质量、生成质量、系统性能
```

---

**下一章 → [附录：术语表、资源、项目](./07-appendix.md)**
