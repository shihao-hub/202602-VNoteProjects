# 模块一：大语言模型（LLM）基础

> 理解 AI 的"大脑"是如何工作的

---

## 第1章：什么是 LLM

### 1.1 LLM 的定义

**LLM（Large Language Model，大语言模型）** 是一种基于深度学习的人工智能系统，通过在海量文本数据上训练，学会理解和生成人类语言。

```
简单来说：
LLM = 大量文本数据 + 复杂的神经网络 + 强大的计算能力
```

### 1.2 历史演进

```
时间线：

2017年 │ Transformer 架构诞生（Google）
       │ - 奠定了现代 LLM 的基础
       ▼
2018年 │ GPT-1 / BERT 发布
       │ - 预训练 + 微调范式确立
       ▼
2019年 │ GPT-2 发布
       │ - 展示了规模化的威力
       ▼
2020年 │ GPT-3 发布（1750亿参数）
       │ - Few-shot Learning 崭露头角
       ▼
2022年 │ ChatGPT 发布
       │ - AI 进入大众视野
       ▼
2023年 │ GPT-4 / Claude / Gemini
       │ - 多模态、更强推理能力
       ▼
2024年 │ 开源模型崛起
       │ - LLaMA、Qwen、DeepSeek 等
       ▼
2025年 │ Agent 时代来临
       │ - LLM 从"能说"到"能做"
```

### 1.3 LLM 与传统程序的本质区别

| 特性 | 传统程序 | LLM |
|------|----------|-----|
| 工作方式 | 按预设规则执行 | 根据学到的模式生成 |
| 输入处理 | 精确匹配 | 模糊理解 |
| 输出结果 | 确定性（相同输入=相同输出） | 概率性（可能有变化） |
| 处理新情况 | 需要修改代码 | 可泛化处理 |
| 可解释性 | 高（可追踪逻辑） | 低（黑盒） |

### 1.4 类比理解

> **LLM 像一个"博学但没有记忆的顾问"**

想象你有一个顾问：
- 📚 **博学**：读过互联网上几乎所有的书、文章、代码
- 🧠 **聪明**：能够理解复杂问题，给出有见地的回答
- 🔄 **无记忆**：每次对话都是全新开始，不记得之前聊过什么
- 🎲 **有创造性**：同一个问题可能给出略有不同的答案

这就是 LLM 的本质特征。

---

## 第2章：LLM 的工作原理

### 2.1 Transformer 架构简介

Transformer 是 LLM 的核心架构。你不需要理解数学细节，只需要知道它的核心思想：

```
核心机制：注意力（Attention）

传统方式：按顺序读文本，一个词一个词处理
Transformer：同时"看到"所有词，并计算它们之间的关系

例子："小明给小红送了一朵花，她很高兴"
      │                      │
      └──── Transformer 能理解"她"指的是"小红" ────┘
```

**为什么 Transformer 重要？**
- 可以并行处理，训练更快
- 能捕捉长距离依赖关系
- 规模化效果好（模型越大，效果越好）

### 2.2 Token 与 Tokenization

**Token** 是 LLM 处理文本的基本单位。

```
Tokenization（分词）示例：

英文：
"Hello, world!" → ["Hello", ",", " world", "!"]

中文：
"你好世界" → ["你好", "世界"] 或 ["你", "好", "世", "界"]

代码：
"print('hello')" → ["print", "('", "hello", "')"]
```

**为什么要了解 Token？**

1. **费用计算**：API 按 Token 数量收费
2. **上下文限制**：模型能处理的 Token 数量有上限
3. **理解模型行为**：模型不是按"词"思考，而是按 Token

```
Token 数量估算（粗略）：
- 英文：1 个单词 ≈ 1-2 个 Token
- 中文：1 个汉字 ≈ 1-2 个 Token
- 代码：通常比自然语言需要更多 Token
```

### 2.3 上下文窗口（Context Window）

**上下文窗口**是 LLM 一次能"看到"的最大 Token 数量。

```
常见模型的上下文窗口（2025年）：

GPT-4 Turbo    │████████████████████████████████│ 128K tokens
Claude 3       │████████████████████████████████████████│ 200K tokens
Gemini 1.5     │████████████████████████████████████████████████│ 1M tokens
LLaMA 3        │████████│ 8K-128K tokens（不同版本）
```

**上下文窗口的意义：**

```
上下文窗口 = 模型的"工作记忆"

太小的窗口：
┌─────────────────────────┐
│ [对话开头被遗忘...]     │
│ ...                     │
│ 最近的几条消息          │ ← 模型只能看到这里
└─────────────────────────┘

足够大的窗口：
┌─────────────────────────┐
│ 系统提示词              │
│ 完整的对话历史          │
│ 参考文档                │
│ 当前问题                │ ← 模型能看到所有内容
└─────────────────────────┘
```

### 2.4 温度（Temperature）与采样

**温度**控制 LLM 输出的随机性/创造性。

```
温度参数范围：0.0 - 2.0（通常）

温度 = 0.0（确定性）
├── 每次输出几乎相同
├── 适合：事实性问答、代码生成、数据处理
└── 比喻：严谨的学者

温度 = 0.7（平衡）
├── 有一定变化但保持连贯
├── 适合：一般对话、写作
└── 比喻：有创意的专业人士

温度 = 1.0+（创造性）
├── 输出更多样、更意外
├── 适合：头脑风暴、创意写作
└── 比喻：天马行空的艺术家
```

### 2.5 类比：用"接龙游戏"理解文本生成

> **LLM 生成文本就像在玩一个超级复杂的文字接龙游戏**

```
用户输入："今天天气"

LLM 内部过程：
1. 看到"今天天气"
2. 根据训练数据，计算下一个词的概率
   - "很好" → 30%
   - "不错" → 25%
   - "晴朗" → 20%
   - "真热" → 15%
   - 其他 → 10%
3. 根据温度参数，选择一个词（假设选了"很好"）
4. 现在变成"今天天气很好"
5. 重复步骤 2-4，继续生成下一个词
6. 直到生成完整回答或达到长度限制
```

**关键理解：**
- LLM 不是"知道"答案，而是在**预测**最可能的下一个词
- 这就是为什么它有时候会"胡说八道"——它只是在做概率预测

---

## 第3章：LLM 的能力与局限

### 3.1 LLM 擅长什么

```
✅ 语言理解与生成
   - 理解复杂文本
   - 生成流畅的自然语言
   - 多语言翻译

✅ 知识问答
   - 回答常识性问题
   - 解释概念
   - 提供背景信息

✅ 文本处理
   - 摘要生成
   - 改写润色
   - 格式转换

✅ 代码相关
   - 代码生成
   - 代码解释
   - Bug 定位与修复建议

✅ 推理与分析
   - 逻辑推理（有一定限制）
   - 问题分析
   - 方案建议

✅ 创意任务
   - 写作辅助
   - 头脑风暴
   - 内容创作
```

### 3.2 LLM 的局限

```
❌ 幻觉（Hallucination）
   - 会自信地说出错误信息
   - 会编造不存在的引用、数据
   - 无法区分自己"知道"和"猜测"的内容

❌ 知识截止
   - 训练数据有时间限制
   - 不知道训练之后发生的事情
   - 某些领域的知识可能过时

❌ 上下文限制
   - 无法处理超长文档
   - 长对话中可能"遗忘"早期内容
   - 在上下文中间的信息可能被忽略

❌ 数学与精确计算
   - 复杂计算容易出错
   - 大数运算不可靠
   - 需要工具辅助

❌ 实时信息
   - 无法访问互联网（除非特别配置）
   - 不知道当前时间、天气等
   - 无法查看最新新闻

❌ 个人信息与状态
   - 没有持久记忆
   - 不真正"认识"用户
   - 每次对话都是独立的
```

### 3.3 为什么 LLM 会"胡说八道"

```
幻觉产生的原因：

1. 概率预测的本质
   ┌──────────────────────────────────────┐
   │ LLM 只是预测"最可能的下一个词"        │
   │ 它不"知道"什么是对的，什么是错的     │
   └──────────────────────────────────────┘

2. 训练数据的问题
   - 互联网上有大量错误信息
   - 模型无法区分真假
   - 学到了一些错误的模式

3. 过度自信
   - 模型被训练成要给出答案
   - 即使不确定，也会生成看起来合理的回答
   - 缺乏"我不知道"的表达能力

4. 模式匹配而非理解
   - 看到类似的模式就套用
   - 可能张冠李戴
```

**实际例子：**

```
用户：谁写了《红楼梦》？
LLM：曹雪芹写了《红楼梦》。 ✅ 正确

用户：谁写了《蓝楼梦》？
LLM：吴承恩写了《蓝楼梦》。 ❌ 幻觉！《蓝楼梦》不存在

原因：模型看到类似的问题模式，就生成了一个"合理"的答案
```

### 3.4 如何正确设定对 LLM 的预期

```
正确的心态：

把 LLM 当作：
✅ 聪明的助手，需要监督
✅ 知识渊博的参考，需要验证
✅ 高效的工具，需要正确使用
✅ 创意的伙伴，需要筛选

不要把 LLM 当作：
❌ 绝对正确的权威
❌ 可以完全信任的信息源
❌ 替代专业判断的决策者
❌ 有自我意识的存在
```

**最佳实践：**

```
1. 重要信息要验证
   - 不要直接引用 LLM 给出的数据、引用、统计
   - 用搜索引擎或官方来源核实

2. 利用 LLM 的优势
   - 作为起点，不是终点
   - 用于头脑风暴、草稿生成
   - 处理重复性的语言任务

3. 明确指令
   - 告诉它你需要什么
   - 指定格式和要求
   - 要求它说明不确定的地方

4. 迭代优化
   - 不满意就追问或重新提问
   - 提供反馈让它改进
   - 多轮对话达到目标
```

---

## 第4章：主流 LLM 介绍

### 4.1 OpenAI GPT 系列

```
┌─────────────────────────────────────────────────┐
│  OpenAI GPT 系列                                │
├─────────────────────────────────────────────────┤
│  GPT-3.5                                        │
│  ├── 速度快、成本低                             │
│  ├── 适合简单任务                               │
│  └── ChatGPT 免费版使用                         │
│                                                 │
│  GPT-4 / GPT-4 Turbo                           │
│  ├── 更强的推理能力                             │
│  ├── 支持图像输入（多模态）                     │
│  ├── 128K 上下文窗口                           │
│  └── 适合复杂任务                               │
│                                                 │
│  GPT-4o                                         │
│  ├── 多模态原生支持                             │
│  ├── 速度更快                                   │
│  └── 性价比更高                                 │
│                                                 │
│  o1 系列                                        │
│  ├── 专注于推理能力                             │
│  ├── 会"思考"后再回答                          │
│  └── 适合数学、编程、科学问题                   │
└─────────────────────────────────────────────────┘

特点：生态最成熟，API 使用广泛
访问：api.openai.com 或 ChatGPT
```

### 4.2 Anthropic Claude 系列

```
┌─────────────────────────────────────────────────┐
│  Anthropic Claude 系列                          │
├─────────────────────────────────────────────────┤
│  Claude 3 Haiku                                 │
│  ├── 最快、最便宜                               │
│  └── 适合简单任务、大批量处理                   │
│                                                 │
│  Claude 3 Sonnet                                │
│  ├── 性能与速度平衡                             │
│  └── 日常使用的好选择                           │
│                                                 │
│  Claude 3 Opus / Claude 3.5 Sonnet             │
│  ├── 最强性能                                   │
│  ├── 出色的长文本处理能力                       │
│  └── 编程能力强                                 │
│                                                 │
│  共同特点                                       │
│  ├── 200K tokens 超长上下文                    │
│  ├── 更安全、更少有害输出                       │
│  └── 遵循指令能力强                             │
└─────────────────────────────────────────────────┘

特点：长上下文、安全性好、编程能力强
访问：claude.ai 或 API
```

### 4.3 Google Gemini

```
┌─────────────────────────────────────────────────┐
│  Google Gemini 系列                             │
├─────────────────────────────────────────────────┤
│  Gemini 1.5 Flash                               │
│  ├── 快速、低成本                               │
│  └── 适合大规模应用                             │
│                                                 │
│  Gemini 1.5 Pro                                 │
│  ├── 高达 1M tokens 上下文（最长！）            │
│  ├── 原生多模态                                 │
│  └── 与 Google 服务集成                         │
│                                                 │
│  Gemini Ultra                                   │
│  ├── 最强性能                                   │
│  └── 复杂推理任务                               │
└─────────────────────────────────────────────────┘

特点：超长上下文、多模态原生、与 Google 生态集成
访问：gemini.google.com 或 Google AI Studio
```

### 4.4 开源模型

```
┌─────────────────────────────────────────────────┐
│  主要开源模型                                   │
├─────────────────────────────────────────────────┤
│  Meta LLaMA 系列                                │
│  ├── LLaMA 3: 8B, 70B 参数版本                 │
│  ├── 开源社区最活跃                             │
│  └── 大量微调版本可用                           │
│                                                 │
│  阿里 Qwen（通义千问）                          │
│  ├── 中文能力出色                               │
│  ├── 多种尺寸可选                               │
│  └── 开源许可友好                               │
│                                                 │
│  DeepSeek                                       │
│  ├── 编程能力突出                               │
│  ├── 成本效益高                                 │
│  └── DeepSeek Coder 专注代码                   │
│                                                 │
│  Mistral                                        │
│  ├── 小尺寸高性能                               │
│  ├── Mixtral MoE 架构创新                       │
│  └── 欧洲 AI 代表                               │
└─────────────────────────────────────────────────┘

开源模型的优势：
✅ 可本地部署，数据隐私
✅ 可自定义微调
✅ 无 API 调用费用（但需要硬件）
✅ 社区支持活跃

开源模型的挑战：
❌ 需要技术能力部署
❌ 需要 GPU 硬件
❌ 性能通常略逊于闭源顶级模型
```

### 4.5 如何选择适合的模型

```
选择决策树：

                    ┌─────────────────┐
                    │ 你的使用场景？   │
                    └────────┬────────┘
                             │
          ┌──────────────────┼──────────────────┐
          │                  │                  │
          ▼                  ▼                  ▼
    ┌──────────┐      ┌──────────┐      ┌──────────┐
    │ 日常对话 │      │ 编程开发 │      │ 专业任务 │
    └────┬─────┘      └────┬─────┘      └────┬─────┘
         │                 │                 │
         ▼                 ▼                 ▼
    ChatGPT/Claude    Claude/GPT-4      根据具体
    都可以            Copilot           需求选择
                      DeepSeek

考虑因素：
┌────────────────────────────────────────────────┐
│ 因素          │ 影响选择                       │
├────────────────────────────────────────────────┤
│ 任务复杂度    │ 简单→快速模型，复杂→强模型    │
│ 预算          │ 免费→ChatGPT，付费→按需选择   │
│ 上下文长度    │ 长文档→Claude/Gemini          │
│ 数据隐私      │ 敏感数据→考虑本地开源模型     │
│ 响应速度      │ 实时应用→选择快速模型         │
│ 特定能力      │ 代码→Claude/DeepSeek          │
│               │ 中文→Qwen/ChatGPT             │
└────────────────────────────────────────────────┘
```

---

## 本章小结

```
核心要点回顾：

1. LLM 是什么
   └── 通过大量数据训练的语言模型，能理解和生成文本

2. 工作原理
   ├── Transformer 架构：并行处理，捕捉词语关系
   ├── Token：文本处理的基本单位
   ├── 上下文窗口：模型的"工作记忆"
   └── 温度：控制输出的随机性

3. 能力与局限
   ├── 擅长：语言理解、生成、翻译、代码、推理
   └── 局限：幻觉、知识截止、上下文限制、精确计算

4. 主流模型
   ├── OpenAI GPT：生态成熟
   ├── Claude：长上下文、安全
   ├── Gemini：超长上下文、多模态
   └── 开源模型：可定制、可本地部署
```

---

**下一章 → [第5章：Prompt 基础概念](./02-prompt-engineering.md)**
